{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Mohan Sai Aravind Kumar\n",
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology:\n",
    "\n",
    "1. Firstly through PyPDF2 package 30001781.pdf file is extracted. Unit codes are stored in a list, synopsis are stored in a list and the outcomes are also stored in list\n",
    "2. For Case Normalization to takes place, I first considered doing sentence tokenization of each synopsis and outcomes list of strings using sent_tokenize and then normalized the word according to the asked question in the task\n",
    "3. After that word tokenization is done on the lists before removing the STOPWORDS.\n",
    "4. As the stopwords removed from the synopsis list and outcomes list, then both are concatenated which means each unitcode's synopsis and outcomes will be in one list stored another list.\n",
    "5. Contexual words are removed but these word tokens are not repeated when compared with threshold frequency i.e 95%, so the words tokens are not removed\n",
    "6. Removal of rare tokens are also done by chain.from_iterable() method and removed just like the stopwords removal from list\n",
    "7. Unigrams and bi grams are generated using 'MWETokenizer'.\n",
    "8. Stemming is done on the final tokens obtained after removal of rare tokens using PorterStemmer.\n",
    "9. Finally generating vocab.txt and countvec.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 # importing PyPDF2 python package to read and extract infromation in the pdf file \n",
    "import re     # importing re package in python to extract strings using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   # natural language toolkit\n",
    "from nltk.tokenize import RegexpTokenizer # it tokenizes the given tokenizer(reg)\n",
    "from nltk.tokenize import sent_tokenize  # it tokenizes each sentence  \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer # remove morphological affixes from words and leaves the stem\n",
    "from nltk.probability import *\n",
    "from __future__ import division\n",
    "from itertools import chain  #chain.from_iterable() method is used to get the list of unique tokens\n",
    "from nltk.util import ngrams # to create unigrams, bigrams, trigrams etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = open('30001781.pdf','rb')    # opening given 30001781.pdf file\n",
    "new_pdf = PyPDF2.PdfFileReader(new_file) # intializes PdfFileReader object\n",
    "#print(new_pdf.getNumPages())    # to get how many pages are in the pdf(object.getNumPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitcodes_list = []  #creating a unit codes list to store all the uit codes of pdf file in a list\n",
    "for i in range(new_pdf.getNumPages()):  #new_pdf.getNumPages gives total number of pages i the pdf file \n",
    "    page_object = new_pdf.getPage(i) # storing each page of information in a page_object\n",
    "    text=page_object.extractText() # page_object.extractText() method extract the infromation in page_object\n",
    "    text=text.split('\\n')\n",
    "    for each in text:\n",
    "        unit_code = re.findall(r'^[A-Z]{3}[0-9]{4}$',each) #regular expression for unitcode to extract from each pdf page\n",
    "        if unit_code:    # findall gives all the matched patterns into a list of unit codes\n",
    "            for item in unit_code:  \n",
    "                unitcodes_list.append(item)  #all the matched patterns appended to list of all the unit codes\n",
    "#len(unitcodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_list = []  # creating a list for synopsis to store each pdf unit codes \n",
    "for items in range(new_pdf.getNumPages()): # new_pdf.getNumPages gives total number of pages items the pdf file \n",
    "    page_object = new_pdf.getPage(items)  # storing each page of information in a page_object\n",
    "    synopsis = re.compile(r'[A-Z]{3,4}[0-9]{4}(.*?)\\[',re.DOTALL) #regular expression for synopsis to extract from each pdf page\n",
    "    match_sys = re.findall(synopsis,page_object.extractText())  # for all the matached pattern , findall finds all those patterns and gives a list of synopsis\n",
    "    if match_sys:\n",
    "        for items in match_sys:    # each macthed synopsis is appended to synopsis_list\n",
    "            synopsis_list.append(items)\n",
    "#len(synopsis_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_list = []  # creating a list for outcomes to store each pdf unit codes \n",
    "for items in range(new_pdf.getNumPages()):  # new_pdf.getNumPages gives total number of pages items the pdf file \n",
    "    page_object = new_pdf.getPage(items)   # storing each page of information in a page_object\n",
    "    outcomes = re.compile(r'[A-Z]{3}[0-9]{4}.*?(\\[.*?\\])',re.DOTALL)  #regular expression for outcomes to extract from each pdf page\n",
    "    match_out = re.findall(outcomes,page_object.extractText())  # for all the matached pattern , findall finds all those patterns gives a list of outcomes\n",
    "    if match_out:\n",
    "        for item in match_out:   # each macthed outcomes is appended to outcomes_list\n",
    "            outcomes_list.append(item)\n",
    "            \n",
    "#len(outcomes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_list = [] # final units_list created\n",
    "for i in range(0,200):   # for each unit code, each synopsis and each outcome is appended to units_list \n",
    "    units_list.append([unitcodes_list[i],synopsis_list[i],outcomes_list[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_syn = []  # created senteneced synopsis list to append all the normalized token and ready for word tokenization.\n",
    "for item in synopsis_list:\n",
    "    text = sent_tokenize(item) # sentence tokenization is done here and forwarded for case normalization.\n",
    "    sent_list=[]\n",
    "    for each in text:\n",
    "        each_list=each.split()\n",
    "        each_list[0] = each_list[0].lower()  # case normalization lower is executed here\n",
    "        joined = ' '.join(each_list)\n",
    "        sent_list.append(joined)\n",
    "    synopsis_sent = '.'.join(sent_list)\n",
    "    sentence_syn.append(synopsis_sent)     \n",
    "\n",
    "sentence_out = []  # created senteneced synopsis list to append all the normalized token and ready for word tokenization.\n",
    "for items in outcomes_list:\n",
    "    text1 = sent_tokenize(items)  # sentence tokenization is done here and forwarded for case normalization.\n",
    "    sent_list1=[]\n",
    "    for each1 in text:\n",
    "        \n",
    "        each_list1=each1.split()\n",
    "        each_list1[0] = each_list1[0].lower()  # case normalization lower is executed here\n",
    "        joined1 = ' '.join(each_list1)\n",
    "        sent_list1.append(joined1)\n",
    "    outcomes_sent = '.'.join(sent_list1)\n",
    "    sentence_out.append(outcomes_sent) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsistokens_list = []  # creating synopsis tokens list for tokenized synopsis list\n",
    "outcomestokens_list = []  # creating outcomes tokens list for tokenized outcomes list\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")   # regular expression for word tokenization \n",
    "for value in sentence_syn:           \n",
    "    tokens1 = tokenizer.tokenize(value)\n",
    "    synopsistokens_list.append(tokens1)  # tokenized synopsis list\n",
    "                                          \n",
    "for item in outcomes_list:\n",
    "    tokens = tokenizer.tokenize(item)\n",
    "    outcomestokens_list.append(tokens)  # tokenized outcomes list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = []   # creating stopwords empty list\n",
    "syns_stop=[]          # creating empty synopsis removed stopwords list\n",
    "outs_stop=[]          # creating empty outcomes removed stopwords list\n",
    "  # creating a empty list for combined synopsis and outcomes stopwords list\n",
    "stopwords_list = open('stopwords_en.txt','r')\n",
    "stopwords_set = set(stopwords_list)\n",
    "stopwords_list1 = []\n",
    "for each in stopwords_set:\n",
    "    each=each.strip().replace('\\n','')\n",
    "    stopwords_list1.append(each)\n",
    "       #converting list of stopwords to set of stopwords\n",
    "for values2 in synopsistokens_list:       \n",
    "    stop_sys_list=[]\n",
    "    for each in values2: \n",
    "        if each not in stopwords_list1:     # removing stop words from tokenized synopsis words \n",
    "            stop_sys_list.append(each)\n",
    "    syns_stop.append(stop_sys_list)       #appending to a list of stopwords removed synopsis tokens  \n",
    "    \n",
    "for values3 in outcomestokens_list:\n",
    "    stop_out_list=[]\n",
    "    for each in values3: \n",
    "        if each not in stopwords_list1:    # removing stop words from tokenized outcomes words \n",
    "            stop_out_list.append(each)\n",
    "    outs_stop.append(stop_out_list)      #appending to a list of stopwords removed outcomes tokens \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated both the list into tokens_sys_out i.e tokens of synopsis and outcomes in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sys_out = [] \n",
    "for i in range(len(syns_stop)):          # integrating both removed synopsis stop words and removed outcomes stop words\n",
    "    tokens_sys_out.append(syns_stop[i]+outs_stop[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent words with respect to document frequency: that is count of how many number of documents a word occurs.\n",
    "Instead iterating loops, we use FreqDist() and  set() as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we use set() to get unique words in the tokens_sys_out\n",
    "[set(value) for value in tokens_sys_out]\n",
    "2. and we use chain.from_iterable to get list of words and then use FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = list(chain.from_iterable([set(value) for value in tokens_sys_out]))\n",
    "fd_2 = FreqDist(words_count)\n",
    "#print(\"most common words:\",fd_2.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above stats for document frequency shows that none of words match to threshold which is set to 95% i.e approximatley 199 words count.\n",
    "None of the above match the count. so we don't remove any words from the tokens list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are checking the term freqency that is count of words occuring in the entire tokes list \n",
    "1. we use chain.from_iterable to gets the words in tokens_sys_out but without using set()\n",
    "2. use FreqDist() method to get the frequency ditribution of each words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_3 = list(chain.from_iterable(tokens_sys_out))  #list of words occuring the document \n",
    "fd_3 = FreqDist(words_3) # frequency ditribution of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unit', 243),\n",
       " ('students', 178),\n",
       " ('research', 174),\n",
       " ('design', 172),\n",
       " ('skills', 146),\n",
       " ('management', 117),\n",
       " ('data', 105),\n",
       " ('analysis', 102),\n",
       " ('development', 100),\n",
       " ('understanding', 97)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_3.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rare Tokens removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As they asked for threshold set to 5%, that comes upto 10 \n",
    "As we found the document frequecy of all the words in the document, we can now eailsy remove rare tokens whose document frequency is less than 10. It can be done as follows\n",
    "1. Firstly, take the rare tokens out and append them to list so that we can remove them from the main list.\n",
    "2. fd_3 is the dictionary of all the document frequencies, so we can remove from that dictionary, the rare tokens by comparing count of each key value to greater than 10 and apped them to list as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_freq_words = []\n",
    "for each1 in fd_3:       # finding the less frequent words and appending to a list\n",
    "    if fd_3[each1] < 10:\n",
    "        less_freq_words.append(each1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, below cell contains code to remove the rare tokens from the tokens_sys_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_sys_out = []         \n",
    "for tokens in tokens_sys_out:               #removing the less frequent words from the list of all the removed stopped tokens\n",
    "    less_sys_out=[]\n",
    "    for each in tokens: \n",
    "        if each not in less_freq_words:\n",
    "            less_sys_out.append(each)\n",
    "    removed_sys_out.append(less_sys_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uni-grams and bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are checking the term freqency that is count of words occuring in the entire tokes list using chain.from_iterable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(chain.from_iterable(value for value in removed_sys_out))\n",
    "bigrams = ngrams(freq, n = 2)  # importing ngrams method\n",
    "fdbigram = FreqDist(bigrams)   # calculating frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(freq)\n",
    "bigrams_list = finder.nbest(bigram_measures.pmi, 200) # creating bigrams for frequency tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A MWETokenizer merges multi_word expressions into single tokens\n",
    "tokenizer = MWETokenizer(bigrams_list)   # so here variable takes MWEToeknizer with bigrams_list and converts into bigrams \n",
    "all_grams_list = []\n",
    "for token in removed_sys_out:\n",
    "    all_grams_list.append(tokenizer.tokenize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  # stemming the using PorterStemmer()\n",
    "stemming_allgrams = []       \n",
    "for items in all_grams_list:\n",
    "    stem_list=[]\n",
    "    for each in items:         #each item in the all_grams_list is stemmed \n",
    "        each=stemmer.stem(each)\n",
    "        stem_list.append(each)\n",
    "    stemming_allgrams.append(stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generating vocab.txt by taking stemming_allgrams list, applying chain.from_iterable() and make a set and sort it out in alphabatically order.\n",
    "and the writing the vocab_file items into the .txt file\n",
    "2. After stemming all the tokens , I converted into sorted list and then checking for the word length tokes less than 3 or equal to 3\n",
    "3. wrtiting the value and index_value to the .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = sorted(set(list(chain.from_iterable(value for value in stemming_allgrams))))\n",
    "for value in vocab_file:      # here removing the tokens with wordLength less than 3 or equal to 3\n",
    "    if len(value) <= 3:       \n",
    "        del vocab_file[vocab_file.index(value)]\n",
    "with open('30001781_vocab.txt','w+') as file:    # opening the file here\n",
    "    for value in vocab_file:\n",
    "        file.write(f'{value}:{vocab_file.index(value)}\\n') # writing the vocab_file items into the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell I created 30001781_countVec.txt to write all the unitcodes,vocab_file.index and stemming_allgrams.count()\n",
    "So I have written all the document frequency values, indecies of particular unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('30001781_countVec.txt','w+') as vec:\n",
    "    for p in range(len(stemming_allgrams)):\n",
    "        vec.write(f'\\n{unitcodes_list[p]},')\n",
    "        for each in stemming_allgrams[p]:\n",
    "            if each in vocab_file:\n",
    "                vec.write(f'{vocab_file.index(each)}:{stemming_allgrams[p].count(each)},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
